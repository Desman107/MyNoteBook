# BP Neural Network

 ## 激活函数

 ### Sigmoid函数
 $$
 \varphi(x) = \frac{1}{1 + e^{-x}}
 $$
+ Sigmoid函数优点在于输出范围有限,数据传播过程不容易发散，并且输出范围时(0,1)，可以在输出层表示概率值
+ Sigmoid函数的主要缺点时梯度下降非常明显，求两头过于平坦，容易出现梯度消失问题

### 双曲正切函数
$$
tanh(x) = \frac{e^x - x^{-x}}{x^{-x} + e^x}
$$
+ 双曲正切函数将数据映射到[-1,1]，解决了sigmoid函数值域不对称的问题。但梯度小时问题仍然存在。

### ReLU函数
$$
R(z) = max(0,z)
$$
+ ReLU函数是神经网络里最常用的激活函数，ReLU函数收敛速度比Sigmoid、Tanh更快，没有梯度饱和问题，只需要一个阈值就可以得到，不需要归一化输入来防止达到饱和

### 激活函数
+ 激活函数经常使用Sigmoid函数、双曲正切函数、ReLU函数
+ 激活函数有以下特征
  + 非线性
  + 可微性
  + 单调性
  + $f(x)\approx x$
  + 输出值范围
  + 计算简单
  + 归一化

## 损失函数
### 交叉熵
+ 使用与分类问题
+ 可以用于目标位[0,1]区间的回归问题
$$
J(w) = \frac{1}{N}\sum_{n=1}^NH(p_n,q_n)=-\frac{1}{N}\sum_{n=1}^N[y_n\log\widehat{y}_n+(1-y_n)\log(1-\widehat{y}_n)]
$$
### 均方误差
+ 训练时一般目标是最小化MSE
$$
MSE = \frac{1}{n}\sum_{i=1}^n(\widehat{y}_i-i_i)^2
$$
## 学习率
+ 学习率控制每次更新参数的幅度，过高何过低的学习率都可能队模型带来不良影响，何时的学习率可以加快模型的训练速度
+ 常见学习率调整方法
  + 经验手动调整
  + 固定学习率
  + 动量法动态调整
  + 随机梯度下降
  + Adam自动调整

## 过拟合
+ 过拟合是指模型在训练集上预测效果好，测试集效果差
+ 防止过拟合的方法有
  + 正则化
  + 数据增强
  + 早停
  + bagging集成方法
  + dropout
  + 批正则化
  
### 惩罚性的成本函数
+ L2正则化，几次方就是几正则化
+ 将惩罚项添加值损失函数
$$
J = \frac{1}{2n}\sum_{i=1}^n(\widehat{y}_i-y_i)^2+\lambda\sum_{j = 1}^mW_j^2
$$
+ 变量$W_J$代表某个连接的权重